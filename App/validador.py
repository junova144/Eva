# app/validador.py
# =====================================================
#  EVA - Cadena Modular de Validaci贸n y Generaci贸n de Prompt
# =====================================================

from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnableSequence
from typing import Dict, Any
import json
import os

# ----------------------------------------------------
# 1. INICIALIZACIN DE COMPONENTES (GLOBAL)
# ----------------------------------------------------
llm_validator = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.2,
    verbose=False  ##(antes True, genera logs innecesarios)
)

parser = StrOutputParser()

# =======================================================================
# 2. DEFINICIN DE CADENAS LCEL
# ======================================================================

########## Cadena (Detecci贸n de Curso)
curso_prompt = ChatPromptTemplate.from_template("""
Eres un analizador de preguntas escolares.

Solo considera estos cursos: Matem谩tica, Comunicaci贸n, Ciencia y Tecnolog铆a, Educaci贸n para el Trabajo, Ingl茅s.

Analiza la siguiente pregunta y devuelve 煤nicamente el curso correspondiente de la lista anterior.
Devuelve solo uno de: Matem谩tica, Comunicaci贸n, Ciencia y Tecnolog铆a, Educaci贸n para el Trabajo, Ingl茅s.

Pregunta: {pregunta}
""")
curso_chain = curso_prompt | llm_validator | parser

# ----------------------------------------------------
# Funci贸n de normalizaci贸n de texto de curso

#  agregado para mejorar coincidencias entre cursos con variantes menores
def normalizar_curso(texto: str) -> str:
    return texto.lower().replace(".", "").strip()

########### Cadena (Contraste / Python Pura)
def generar_contraste_binario_estructurado(datos: Dict[str, Any]) -> Dict[str, Any]:
    """
    Compara solo la coherencia del curso. Asume que la intenci贸n siempre es GUIA.
    """
    curso_sistema = datos.get("curso_sistema", "")
    curso_detectado = datos.get("curso_detectado", "")

    curso_coincide = (normalizar_curso(curso_sistema) == normalizar_curso(curso_detectado))
    valido = curso_coincide  # La pregunta es v谩lida si el curso es correcto.

    # --- Generaci贸n de C贸digo de Error ---
    if valido:
        codigo_error = "OK"
        mensaje_base = "OK"
    else:
        codigo_error = "ERROR_CURSO_INCORRECTO"
        mensaje_base = "CURSO_INCORRECTO"

    return {
        "valido": valido,
        "codigo_error": codigo_error,
        "mensaje_base": mensaje_base,
        **datos  # Devolvemos todo el contexto
    }

contraste_chain = RunnableLambda(generar_contraste_binario_estructurado)

########## Cadena 4 (Generador de Prompt Final)
prompt_especializado = ChatPromptTemplate.from_template("""
ROL: Eres un generador de respuestas finales del sistema LuzIA. Tu NICA FUNCIN es formatear el resultado basado en el diagn贸stico.

INSTRUCCIONES CRTICAS:
1. Devuelve el resultado final en el formato JSON requerido.
2. Solo se admite el fallo por curso incorrecto.

DATOS DE ENTRADA:
- Validez: {valido}
- Tipo de Fallo: {mensaje_base}
- Curso del sistema: {curso_sistema}
- Curso detectado: {curso_detectado}
- Pregunta original: {entrada_usuario}

------------------------------
1锔 Si **valido es False**:
   Genera una respuesta breve y amable dirigida al usuario.
   
   - SI **Tipo de Fallo es CURSO_INCORRECTO** (el 煤nico fallo posible): 
        Devuelve el texto: 
        "La pregunta no corresponde al curso de **{curso_sistema}**. 
        Fue clasificada como **{curso_detectado}**. 
        Por favor, reformula tu pregunta dentro del contexto de **{curso_sistema}**."
------------------------------
2锔 Si **valido es True** (Si mensaje_base es 'OK'):
   Genera una instrucci贸n de m谩quina limpia para el agente LLM especialista:
   [COMANDO_AGENTE]
   ANALIZA_TEMA: {entrada_usuario}
   CONTEXTO_EDUCATIVO: {curso_sistema}
   ACCIN: Generar respuesta pedag贸gica, clara y precisa.
------------------------------
""")  #  REEMPLAZADO (corrige los placeholders de {curso_sistema}/{curso_detectado})

generar_prompt_agente = prompt_especializado | llm_validator | parser

# =======================================================================
# 3. COMPILACIN DEL PIPELINE GLOBAL
# =======================================================================

#  RunnableLambda (una sola tarea)
deteccion_chain = RunnableLambda(
    lambda x: {"curso_detectado": curso_chain.invoke({"pregunta": x["entrada_usuario"]}).strip()}
)

pipeline_decision = RunnableSequence(
    RunnableLambda(lambda x: {**x, **deteccion_chain.invoke(x)}),
    contraste_chain
)

# =======================================================================
# 4. FUNCIN WRAPPER FINAL
# =======================================================================
def run_eva_pipeline(grado_sistema: str, curso_sistema: str, pregunta: str) -> Dict:
    """
    Ejecuta el pipeline LCEL simplificado (C2 -> C3) y luego la Cadena 4.
    """
    input_pipeline = {
        "entrada_usuario": pregunta,
        "grado_sistema": grado_sistema,
        "curso_sistema": curso_sistema,
    }

    # 1. Ejecutar la Detecci贸n y Contraste (C2 -> C3)
    resultado_decision = pipeline_decision.invoke(input_pipeline)

    # 2. Ejecutar la Generaci贸n Final (Cadena 4)
    texto_final = generar_prompt_agente.invoke(resultado_decision).strip()

    # 3. Formatear la Salida para el sistema
    es_valido = resultado_decision.get("valido", False)

    if es_valido:
        validacion = {"valido": True, "mensaje": "Validaci贸n exitosa."}
        prompt_final = texto_final
    else:
        validacion = {"valido": False, "mensaje": texto_final}
        prompt_final = ""

    #  REEMPLAZADO: devuelve el JSON como objeto, no string, m谩s manejable desde Streamlit
    return {
        "prompt_final": prompt_final,
        "validacion": validacion,
        "curso_final": curso_sistema
    }

# =====================================================
# FIN DEL MDULO
# =====================================================
